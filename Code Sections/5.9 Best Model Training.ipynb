{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maddi007-Py/Maddi007-Py-CrimeAnalytics_Clustering/blob/main/Code%20Sections/5.9%20Best%20Model%20Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b2eae1",
      "metadata": {
        "id": "05b2eae1",
        "papermill": {
          "duration": 0.001321,
          "end_time": "2025-03-17T03:25:05.307251",
          "exception": false,
          "start_time": "2025-03-17T03:25:05.305930",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### **5.9 Best Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "df8508d3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-17T03:25:05.312232Z",
          "iopub.status.busy": "2025-03-17T03:25:05.311746Z",
          "iopub.status.idle": "2025-03-17T03:40:11.403461Z",
          "shell.execute_reply": "2025-03-17T03:40:11.402328Z"
        },
        "id": "df8508d3",
        "outputId": "9f887dc7-f1d6-4603-a29c-5e349f7e0011",
        "papermill": {
          "duration": 906.097066,
          "end_time": "2025-03-17T03:40:11.405999",
          "exception": false,
          "start_time": "2025-03-17T03:25:05.308933",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
              "         Clustering results saved as <span style=\"color: green;\">Best_clustering_Models.csv </span>.\n",
              "    </p>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from IPython.display import display, HTML\n",
        "import ast  # For safely evaluating strings\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://raw.githubusercontent.com/Maddi007-Py/Maddi007-Py-CrimeAnalytics_Clustering/refs/heads/main/Output_CSV/FE_Encoded.csv\"\n",
        "original_data = pd.read_csv(url)\n",
        "\n",
        "# Store _id before clustering\n",
        "original_data['_id'] = original_data.index\n",
        "\n",
        "# Use 20% of the data (random sample)\n",
        "sample_data = original_data.copy()\n",
        "\n",
        "# Load feature combinations\n",
        "url1 = \"https://raw.githubusercontent.com/Maddi007-Py/Maddi007-Py-CrimeAnalytics_Clustering/refs/heads/main/Output_CSV/Feature_Combo_Current_Results.csv\"\n",
        "feature_combos = pd.read_csv(url1)\n",
        "\n",
        "# Define the set names to match\n",
        "set_names = ['4_Set_165','4_Set_369', '4_Set_490', '4_Set_494', '4_Set_495']\n",
        "\n",
        "# Initialize an empty list to hold the feature sets\n",
        "feature_sets = []\n",
        "\n",
        "# Extract corresponding feature sets\n",
        "for set_name in set_names:\n",
        "    matched_features = feature_combos[feature_combos['Feature Set'] == set_name]['Feature_Names_String']\n",
        "    if not matched_features.empty:\n",
        "        features_list = ast.literal_eval(matched_features.values[0])  # Convert string to list\n",
        "        feature_sets.append(features_list)\n",
        "\n",
        "# Create a copy of the original data to store clustering results\n",
        "clustering_results = original_data.copy()\n",
        "\n",
        "# Add placeholder columns for clustering results\n",
        "for i in range(1, 5):\n",
        "    clustering_results[f'KMeans{i}_Cluster'] = -1\n",
        "    clustering_results[f'KMeans{i}_Silhouette_Score'] = np.nan\n",
        "    clustering_results[f'KMeans{i}_Davies_Bouldin_Index'] = np.nan\n",
        "    clustering_results[f'KMeans{i}_Calinski_Harabasz_Score'] = np.nan\n",
        "    clustering_results[f'KMeans{i}_Prediction_Accuracy'] = np.nan\n",
        "    clustering_results[f'DBSCAN{i}_Cluster'] = -1\n",
        "    clustering_results[f'DBSCAN{i}_Silhouette_Score'] = np.nan\n",
        "    clustering_results[f'DBSCAN{i}_Davies_Bouldin_Index'] = np.nan\n",
        "    clustering_results[f'DBSCAN{i}_Prediction_Accuracy'] = np.nan\n",
        "\n",
        "# Perform clustering on each feature set\n",
        "for i, features in enumerate(feature_sets, start=1):\n",
        "    valid_features = [f for f in features if f in sample_data.columns]\n",
        "    data_for_clustering = sample_data[valid_features].copy()\n",
        "\n",
        "    # Store _id for mapping back\n",
        "    sample_ids = sample_data['_id'].values\n",
        "\n",
        "    numerical_cols = data_for_clustering.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = pd.DataFrame(scaler.fit_transform(data_for_clustering[numerical_cols]), columns=numerical_cols)\n",
        "\n",
        "    # KMeans Clustering\n",
        "    kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "    kmeans_labels = kmeans.fit_predict(data_scaled)\n",
        "    silhouette_score_kmeans = silhouette_score(data_scaled, kmeans_labels)\n",
        "    davies_bouldin_score_kmeans = davies_bouldin_score(data_scaled, kmeans_labels)\n",
        "    calinski_harabasz_score_kmeans = calinski_harabasz_score(data_scaled, kmeans_labels)\n",
        "    kmeans_accuracy = max(0, silhouette_score_kmeans) * 100\n",
        "\n",
        "    # DBSCAN Clustering\n",
        "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "    dbscan_labels = dbscan.fit_predict(data_scaled)\n",
        "    silhouette_score_dbscan = -1 if len(set(dbscan_labels)) <= 1 else silhouette_score(data_scaled, dbscan_labels)\n",
        "    davies_bouldin_score_dbscan = -1 if len(set(dbscan_labels)) <= 1 else davies_bouldin_score(data_scaled, dbscan_labels)\n",
        "    dbscan_accuracy = max(0, silhouette_score_dbscan) * 100\n",
        "\n",
        "    # Update clustering results in the original dataset based on _id matching\n",
        "    for idx, original_idx in enumerate(sample_ids):\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Cluster'] = kmeans_labels[idx]\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Silhouette_Score'] = silhouette_score_kmeans\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Davies_Bouldin_Index'] = davies_bouldin_score_kmeans\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Calinski_Harabasz_Score'] = calinski_harabasz_score_kmeans\n",
        "        clustering_results.loc[original_idx, f'KMeans{i}_Prediction_Accuracy'] = kmeans_accuracy\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Cluster'] = dbscan_labels[idx]\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Silhouette_Score'] = silhouette_score_dbscan\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Davies_Bouldin_Index'] = davies_bouldin_score_dbscan\n",
        "        clustering_results.loc[original_idx, f'DBSCAN{i}_Prediction_Accuracy'] = dbscan_accuracy\n",
        "\n",
        "# Save clustering results to CSV\n",
        "clustering_results.to_csv('Best_clustering_Models.csv', index=False)\n",
        "# Display formatted message for saved file\n",
        "display(HTML(\"\"\"\n",
        "    <p style=\"color: darkblue; font-size: 18px; font-weight: bold;\">\n",
        "         Clustering results saved as <span style=\"color: green;\">Best_clustering_Models.csv </span>.\n",
        "    </p>\n",
        "\"\"\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6864871,
          "sourceId": 11024928,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 910.526388,
      "end_time": "2025-03-17T03:40:12.834817",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-03-17T03:25:02.308429",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}