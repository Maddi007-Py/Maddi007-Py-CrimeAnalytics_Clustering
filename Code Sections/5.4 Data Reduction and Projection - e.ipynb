{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7rCCTuhb2PH9AeMHU8vE6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maddi007-Py/Maddi007-Py-CrimeAnalytics_Clustering/blob/main/5.4%20Data%20Reduction%20and%20Projection%20-%20e)%20EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.4 Data Reduction and Projection - e) EDA**"
      ],
      "metadata": {
        "id": "5U4dfKU4Hhtl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hYg1sseHB1k"
      },
      "outputs": [],
      "source": [
        "# Reading from the Processed Data \"Final_Data.csv\"\n",
        "df = pd.read_csv('Final_Data.csv')\n",
        "\n",
        "# Calculate the frequency of incidents at each location (rounded to 4 decimal places)\n",
        "df['Geo_Location'] = df['LONG_WGS84'].round(7).astype(str) + \", \" + df['LAT_WGS84'].round(7).astype(str)\n",
        "location_counts = df['Geo_Location'].value_counts()\n",
        "\n",
        "# Map frequencies back to the original dataset for color assignment\n",
        "df['Location_Frequency'] = df['Geo_Location'].map(location_counts)\n",
        "\n",
        "# Define custom boundaries for the color mapping\n",
        "boundaries = [0, 30, 250, 350]\n",
        "colors = ['#006400', '#FFFF00', '#FF6666', '#8B0000']  # Green -> Yellow -> Light Red -> Dark Red\n",
        "cmap = LinearSegmentedColormap.from_list(\"custom_green_yellow_red\", colors, N=256)\n",
        "norm = BoundaryNorm(boundaries, cmap.N)\n",
        "\n",
        "# Scatter plot with color based on location frequency\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and axis\n",
        "scatter = sns.scatterplot(x='LONG_WGS84', y='LAT_WGS84', data=df, hue='Location_Frequency', palette=cmap,\n",
        "                          size='Location_Frequency', sizes=(20, 200), alpha=0.6, legend=None,\n",
        "                          hue_norm=norm, ax=ax)\n",
        "ax.set_title(\"Geographic Distribution of Incidents (Color Coded by Frequency)\")\n",
        "ax.set_xlabel(\"Longitude\")\n",
        "ax.set_ylabel(\"Latitude\")\n",
        "\n",
        "# Create a colorbar with the custom colormap and boundaries\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "sm.set_array([])  # Empty array for the mappable to allow colorbar creation\n",
        "fig.colorbar(sm, ax=ax, label=\"Frequency of Incidents\")  # Attach colorbar to the figure and axis\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top 10 most frequent locations (rounded to 4 decimal places)\n",
        "print(\"\\n===== Top 10 Most Frequent Incident Locations =====\\n\")\n",
        "print(location_counts.head(10).to_string())\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Reading from the Processed Data \"Final_Data.csv\"\n",
        "df = pd.read_csv('Final_Data.csv')\n",
        "\n",
        "# Drop unwanted columns\n",
        "df_cleaned = df.drop(columns=['_id', 'EVENT_UNIQUE_ID', 'reporting_delay_days', 'HOOD_158'])\n",
        "\n",
        "# Convert 'OCC_MONTH' and 'OCC_DOW' to categorical variables, treating them as categories\n",
        "df_cleaned['OCC_MONTH'] = pd.Categorical(df_cleaned['OCC_MONTH'], categories=[\n",
        "    'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August',\n",
        "    'September', 'October', 'November', 'December'], ordered=False)\n",
        "\n",
        "df_cleaned['OCC_DOW'] = pd.Categorical(df_cleaned['OCC_DOW'], categories=[\n",
        "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ordered=False)\n",
        "\n",
        "# Now we will only select numerical columns to compute the correlation matrix\n",
        "df_numerical = df_cleaned.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = df_numerical.corr()\n",
        "\n",
        "# Print the textual correlation matrix\n",
        "print(\"\\n===== Correlation Matrix =====\")\n",
        "print(correlation_matrix)\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Plot the heatmap of the correlation matrix\n",
        "plt.figure(figsize=(12, 8))  # Set figure size\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, cbar=True)\n",
        "plt.title(\"Correlation Matrix (Excluding Specific Columns)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Load the current dataset from Final_Data.csv\n",
        "df = pd.read_csv('Final_Data.csv')  # Ensure it's the correct file path\n",
        "\n",
        "# Drop unwanted columns (from previous code)\n",
        "df_cleaned = df.drop(columns=['_id', 'EVENT_UNIQUE_ID', 'reporting_delay_days', 'HOOD_158'])\n",
        "\n",
        "# Set the style for seaborn\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Function to detect outliers, generate visualizations, and provide textual output for scatter plots\n",
        "def detect_outliers(df):\n",
        "    # Select numeric columns from the dataframe\n",
        "    numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "    # Initialize a DataFrame to store outlier information\n",
        "    outlier_summary = pd.DataFrame(index=numeric_df.columns, columns=['Outlier Count', 'Outlier Percentage'])\n",
        "\n",
        "    # Calculate outlier summary for each numeric column (using Z-score method with threshold 3)\n",
        "    for col in numeric_df.columns:\n",
        "        z_scores = np.abs(stats.zscore(numeric_df[col]))\n",
        "        outliers = (z_scores > 3)\n",
        "        outlier_count = outliers.sum()\n",
        "        outlier_percentage = (outlier_count / len(numeric_df)) * 100\n",
        "        outlier_summary.loc[col] = [outlier_count, outlier_percentage]\n",
        "\n",
        "    # Display the outlier summary as text output for easy copy-pasting\n",
        "    print(\"\\n===== Outlier Summary =====\")\n",
        "    for col in numeric_df.columns:\n",
        "        outlier_count = outlier_summary.loc[col, 'Outlier Count']\n",
        "        outlier_percentage = outlier_summary.loc[col, 'Outlier Percentage']\n",
        "        print(f\"{col}: {outlier_count} outliers ({outlier_percentage:.2f}%)\")\n",
        "\n",
        "    # Additional visualizations: Scatter Plots for 2D relationships between numeric columns\n",
        "    num_cols = len(numeric_df.columns)\n",
        "    num_plots = (num_cols * (num_cols - 1)) // 2  # Calculate total pairwise scatter plots\n",
        "    n_rows = num_plots // 3 + (num_plots % 3 > 0)  # Calculate the number of rows needed for the grid\n",
        "\n",
        "    # Set up the figure for scatter plots\n",
        "    plt.figure(figsize=(18, 6 * n_rows))\n",
        "    plot_index = 1\n",
        "\n",
        "    # List to collect text summaries for each scatter plot\n",
        "    scatter_texts = []\n",
        "\n",
        "    for i, col1 in enumerate(numeric_df.columns):\n",
        "        for j, col2 in enumerate(numeric_df.columns):\n",
        "            if col1 != col2 and j > i:  # Ensure that each pair is plotted only once\n",
        "                plt.subplot(n_rows, 3, plot_index)\n",
        "                sns.scatterplot(data=numeric_df, x=col1, y=col2, alpha=0.6)\n",
        "                plt.title(f'Scatter Plot: {col1} vs {col2}')\n",
        "                plt.xlabel(col1)\n",
        "                plt.ylabel(col2)\n",
        "\n",
        "                # Calculate the Pearson correlation coefficient for this pair\n",
        "                correlation = numeric_df[col1].corr(numeric_df[col2])\n",
        "                summary_text = f\"Scatter Plot: {col1} vs {col2} | Pearson Correlation: {correlation:.2f}\"\n",
        "                scatter_texts.append(summary_text)\n",
        "                # Removed the print here to avoid duplicate printing\n",
        "\n",
        "                plot_index += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print a final summary of all scatter plot details (printed only once)\n",
        "    print(\"\\n===== Scatter Plot Summaries =====\")\n",
        "    for text in scatter_texts:\n",
        "        print(text)\n",
        "\n",
        "# Call the function with your cleaned dataframe\n",
        "detect_outliers(df_cleaned)\n",
        "\n",
        "print(\"Outlier detection, visualizations, and correlation calculations completed.\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Final_Data.csv')  # Ensure it's the correct file path\n",
        "\n",
        "# Check each column's unique values\n",
        "for column in df.columns:\n",
        "    print(f\"Feature: {column}\")\n",
        "    print(f\"Unique Values: {df[column].unique()[:10]}\")  # Show the first 10 unique values\n",
        "    print(f\"Total Unique Values: {len(df[column].unique())}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Ensure columns exist in the DataFrame\n",
        "if 'LOCATION_TYPE' not in df.columns or 'PREMISES_TYPE' not in df.columns:\n",
        "    raise ValueError(\"Columns 'LOCATION_TYPE' or 'PREMISES_TYPE' are missing in the dataset.\")\n",
        "\n",
        "# Group LOCATION_TYPE by each unique PREMISES_TYPE\n",
        "grouped_data = df.groupby('PREMISES_TYPE')['LOCATION_TYPE'].unique().reset_index()\n",
        "\n",
        "# Print the results\n",
        "for index, row in grouped_data.iterrows():\n",
        "    print(f\"PREMISES_TYPE: {row['PREMISES_TYPE']}\")\n",
        "    print(\"LOCATION_TYPE values:\")\n",
        "    for loc in row['LOCATION_TYPE']:\n",
        "        print(f\"  - {loc}\")\n",
        "    print(\"-\" * 50)  # Separator for readability\n",
        "\n",
        "# Optional: Save the grouped data to a CSV file\n",
        "#grouped_data.to_csv(\"Grouped_Location_Premises.csv\", index=False)\n"
      ]
    }
  ]
}
