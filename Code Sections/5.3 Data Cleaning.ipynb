{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb61+a3stQpA0SS1Oh+CUJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maddi007-Py/Maddi007-Py-CrimeAnalytics_Clustering/blob/main/Code%20Sections/5.3%20Data%20Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.3 Data Cleaning**"
      ],
      "metadata": {
        "id": "3ZLw2yyJZi-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ts0R-BYUZa-Q",
        "outputId": "3154986d-35b5-4e44-cc96-90a49153a660"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "EmptyDataError",
          "evalue": "No columns to parse from file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-871dcd974b5b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Read the data from CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/Maddi007-Py/Maddi007-Py-CrimeAnalytics_Clustering/main/Output_CSV/Target_Dataset.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mData_Preparing_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import files\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Read the data from CSV file\n",
        "url = \"https://raw.githubusercontent.com/Maddi007-Py/Maddi007-Py-CrimeAnalytics_Clustering/main/Output_CSV/Target_Dataset.csv\"\n",
        "Data_Preparing_df = pd.read_csv(url, low_memory=False).copy()\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "# Output File Name for HTML summary changed to \"5.3 Data Cleaning\"\n",
        "html_output_filename = '/content/5.3 Data Cleaning.html'\n",
        "\n",
        "# Table to store results\n",
        "steps_summary = []\n",
        "\n",
        "# Step 1: Dataset Loading\n",
        "before_step_1 = Data_Preparing_df.shape[0]\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 1: Load Dataset\",\n",
        "    \"Before Action\": before_step_1,\n",
        "    \"Affected by Action\": 0,\n",
        "    \"After Action\": before_step_1,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "# Step 2: Identify and remove true duplicates (excluding '_id')\n",
        "columns_to_check = [col for col in Data_Preparing_df.columns if col != '_id']\n",
        "duplicate_count = Data_Preparing_df.duplicated(subset=columns_to_check).sum()\n",
        "rows_before_dedup = Data_Preparing_df.shape[0]\n",
        "Data_Preparing_df = Data_Preparing_df.drop_duplicates(subset=columns_to_check, keep='first').copy()\n",
        "rows_after_dedup = Data_Preparing_df.shape[0]\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 2: Remove TRUE DUPLICATE Records\",\n",
        "    \"Before Action\": rows_before_dedup,\n",
        "    \"Affected by Action\": duplicate_count,\n",
        "    \"After Action\": rows_after_dedup,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "# Step 3: Drop rows with null, NaN, or missing data\n",
        "before_drop_rows = Data_Preparing_df.shape[0]\n",
        "Data_Preparing_df = Data_Preparing_df.dropna().copy()\n",
        "after_drop_rows = Data_Preparing_df.shape[0]\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 3: Drop Rows with Missing Data\",\n",
        "    \"Before Action\": before_drop_rows,\n",
        "    \"Affected by Action\": before_drop_rows - after_drop_rows,\n",
        "    \"After Action\": after_drop_rows,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "# Step 4: Strip leading and trailing spaces from string columns\n",
        "obj_cols = Data_Preparing_df.select_dtypes(include=\"object\").columns\n",
        "orig_step4 = Data_Preparing_df[obj_cols].copy()\n",
        "Data_Preparing_df[obj_cols] = Data_Preparing_df[obj_cols].apply(lambda s: s.str.strip())\n",
        "affected_step4 = (orig_step4 != Data_Preparing_df[obj_cols]).any(axis=1).sum()\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 4: Strip Leading/Trailing Spaces\",\n",
        "    \"Before Action\": after_drop_rows,\n",
        "    \"Affected by Action\": affected_step4,\n",
        "    \"After Action\": after_drop_rows,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "# Step 5: Remove leading apostrophes from string columns\n",
        "orig_step5 = Data_Preparing_df[obj_cols].copy()\n",
        "Data_Preparing_df[obj_cols] = Data_Preparing_df[obj_cols].apply(lambda s: s.str.lstrip(\"'\"))\n",
        "affected_step5 = (orig_step5 != Data_Preparing_df[obj_cols]).any(axis=1).sum()\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 5: Remove Leading Apostrophes\",\n",
        "    \"Before Action\": after_drop_rows,\n",
        "    \"Affected by Action\": affected_step5,\n",
        "    \"After Action\": after_drop_rows,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "# Step 6: Match rows where 'HOOD_158' is 'NSA' and replace with matching value based on coordinates\n",
        "nsa_replaced_count = 0\n",
        "for i, row in Data_Preparing_df[Data_Preparing_df['HOOD_158'] == 'NSA'].iterrows():\n",
        "    match = Data_Preparing_df[\n",
        "        (Data_Preparing_df['LONG_WGS84'] == row['LONG_WGS84']) &\n",
        "        (Data_Preparing_df['LAT_WGS84'] == row['LAT_WGS84']) &\n",
        "        (Data_Preparing_df['HOOD_158'] != 'NSA')\n",
        "    ]\n",
        "    if not match.empty:\n",
        "        matched_value = match.iloc[0]['HOOD_158']\n",
        "        Data_Preparing_df.loc[i, 'HOOD_158'] = matched_value\n",
        "        nsa_replaced_count += 1\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 6: Match & Replace 'NSA' Values\",\n",
        "    \"Before Action\": after_drop_rows,\n",
        "    \"Affected by Action\": nsa_replaced_count,\n",
        "    \"After Action\": after_drop_rows,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "# Step 7: Remove remaining rows where 'HOOD_158' or 'NEIGHBOURHOOD_158' contains 'NSA'\n",
        "mask_remaining_nsa = (Data_Preparing_df['HOOD_158'] == 'NSA') | (Data_Preparing_df['NEIGHBOURHOOD_158'] == 'NSA')\n",
        "remaining_nsa_count = mask_remaining_nsa.sum()\n",
        "before_removal = Data_Preparing_df.shape[0]\n",
        "Data_Preparing_df = Data_Preparing_df[~mask_remaining_nsa].copy()\n",
        "after_removal = Data_Preparing_df.shape[0]\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 7: Remove Remaining 'NSA' Rows\",\n",
        "    \"Before Action\": before_removal,\n",
        "    \"Affected by Action\": remaining_nsa_count,\n",
        "    \"After Action\": after_removal,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "# Step 8: Format Longitude & Latitude to 7 decimals\n",
        "orig_long = Data_Preparing_df['LONG_WGS84'].copy()\n",
        "orig_lat = Data_Preparing_df['LAT_WGS84'].copy()\n",
        "Data_Preparing_df.loc[:, 'LONG_WGS84'] = Data_Preparing_df['LONG_WGS84'].astype(float).map(lambda x: f\"{x:.7f}\")\n",
        "Data_Preparing_df.loc[:, 'LAT_WGS84'] = Data_Preparing_df['LAT_WGS84'].astype(float).map(lambda x: f\"{x:.7f}\")\n",
        "affected_step8 = ((orig_long != Data_Preparing_df['LONG_WGS84']) | (orig_lat != Data_Preparing_df['LAT_WGS84'])).sum()\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Step 8: Format Longitude & Latitude to 7 Decimals\",\n",
        "    \"Before Action\": after_removal,\n",
        "    \"Affected by Action\": affected_step8,\n",
        "    \"After Action\": after_removal,\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "# Final row: Rows Affected in 5.3 Data Cleaning\n",
        "final_row_count = Data_Preparing_df.shape[0]\n",
        "steps_summary.append({\n",
        "    \"Step Taken\": \"Rows Affected in <strong>5.3 Data Cleaning </strong>\",\n",
        "    \"Before Action\": \"Initial Load:<br><strong>\" + str(before_step_1) + \"</strong>\",\n",
        "    \"Affected by Action\": \"Overall Reduction:<br><strong>\" + str(before_step_1 - final_row_count) + \"</strong>\",\n",
        "    \"After Action\": \"Final Count:<br><strong>\" + str(final_row_count) + \"</strong>\",\n",
        "    \"Unit\": \"Rows\"\n",
        "})\n",
        "\n",
        "# Build HTML Table with styling\n",
        "html_table = \"\"\"\n",
        "<table style='border-collapse: collapse; width: 100%; font-size: 18px;'>\n",
        "    <thead style='background-color: #4CAF50; color: white;'>\n",
        "        <tr>\n",
        "            <th colspan=\"5\" style=\"text-align: center; font-size: 24px; background-color: #2f4f4f; color: white;\">\n",
        "                5.3 Data Cleaning\n",
        "            </th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th>Step Taken</th>\n",
        "            <th>Before Action</th>\n",
        "            <th>Affected by Action</th>\n",
        "            <th>After Action</th>\n",
        "            <th>Unit</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "\"\"\"\n",
        "for step in steps_summary:\n",
        "    html_table += f\"\"\"\n",
        "        <tr style='border: 1px solid #dddddd;'>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Step Taken']}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Before Action']}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Affected by Action']}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['After Action']}</td>\n",
        "            <td style='border: 1px solid #dddddd; padding: 8px;'>{step['Unit']}</td>\n",
        "        </tr>\n",
        "    \"\"\"\n",
        "# Add final note row spanning all columns with the required note text\n",
        "note_text = (\n",
        "    \"<strong>Note: \"\n",
        "    \"Longitude and Latitude were reduced to 7 Decimal Places as,<br>\"\n",
        "    \"• 7 decimal places offer precision of <span style='color: green;'>1.1 cm</span>, precise enough for GPS devices.<br>\"\n",
        "    \"• Further granularity adds processing time and energy consumption without real-world benefits.<br>\"\n",
        "    \"The final cleaned data has been saved as <span style='color: blue;'> 'Cleaned_Data.csv' </span> for further analysis.</strong>\"\n",
        ")\n",
        "html_table += f\"\"\"\n",
        "        <tr style='border: 1px solid #dddddd;'>\n",
        "            <td colspan=\"5\" style='border: 1px solid #dddddd; padding: 8px;'>{note_text}</td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n",
        "\"\"\"\n",
        "\n",
        "# Save the cleaned data\n",
        "Data_Preparing_df.to_csv('Cleaned_Data.csv', index=False)\n",
        "\n",
        "# Display the HTML table\n",
        "display(HTML(html_table))\n",
        "\n",
        "# Write the HTML file and trigger download\n",
        "with open(html_output_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(html_table)\n",
        "files.download(html_output_filename)\n",
        "print(\"\\n\\n\")"
      ]
    }
  ]
}